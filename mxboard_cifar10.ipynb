{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import signal\n",
    "\n",
    "class TensorBoardServer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def start(self):\n",
    "        self.process = subprocess.Popen(\"tensorboard --logdir ./logs --host 127.0.0.1 --port 6006\",\n",
    "                                  shell=True, preexec_fn=os.setsid)\n",
    "\n",
    "    def stop(self):\n",
    "        os.killpg(self.process.pid, signal.SIGTERM)\n",
    "        \n",
    "tb_server = TensorBoardServer()\n",
    "tb_server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "from mxboard import SummaryWriter\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "transform_fn = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = mx.gluon.data.vision.CIFAR10(train=True).transform_first(transform_fn)\n",
    "train_dataloader = mx.gluon.data.DataLoader(train_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = mx.gluon.data.vision.CIFAR10(train=False).transform_first(transform_fn)\n",
    "test_dataloader = mx.gluon.data.DataLoader(test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.HybridBlock):\n",
    "    \"\"\"\n",
    "    Pre-activation Residual Block with 2 convolution layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, stride=1, dim_match=True):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.dim_match = dim_match\n",
    "        with self.name_scope():\n",
    "            self.bn1 = nn.BatchNorm(epsilon=2e-5)\n",
    "            self.conv1 = nn.Conv2D(channels=channels, kernel_size=3, padding=1, strides=stride, use_bias=False)\n",
    "            self.bn2 = nn.BatchNorm(epsilon=2e-5)\n",
    "            self.conv2 = nn.Conv2D(channels=channels, kernel_size=3, padding=1, strides=1, use_bias=False)\n",
    "            if not self.dim_match:\n",
    "                self.conv3 = nn.Conv2D(channels=channels, kernel_size=1, padding=0, strides=stride, use_bias=False)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        act1 = F.relu(self.bn1(x))\n",
    "        act2 = F.relu(self.bn2(self.conv1(act1)))\n",
    "        out = self.conv2(act2)\n",
    "        if self.dim_match:\n",
    "            shortcut = x\n",
    "        else:\n",
    "            shortcut = self.conv3(act1)\n",
    "        return out + shortcut\n",
    "\n",
    "\n",
    "class ResNet(nn.HybridBlock):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # data normalization\n",
    "            net.add(nn.BatchNorm(epsilon=2e-5, scale=True))\n",
    "            # pre-stage\n",
    "            net.add(nn.Conv2D(channels=16, kernel_size=3, strides=1, padding=1, use_bias=False))\n",
    "            # Stage 1 (4 total)\n",
    "            net.add(BasicBlock(16, stride=1, dim_match=False))\n",
    "            for _ in range(3):\n",
    "                net.add(BasicBlock(16, stride=1, dim_match=True))\n",
    "            # Stage 2 (4 total)\n",
    "            net.add(BasicBlock(32, stride=2, dim_match=False))\n",
    "            for _ in range(3):\n",
    "                net.add(BasicBlock(32, stride=1, dim_match=True))\n",
    "            # Stage 3 (4 in total)\n",
    "            net.add(BasicBlock(64, stride=2, dim_match=False))\n",
    "            for _ in range(3):\n",
    "                net.add(BasicBlock(64, stride=1, dim_match=True))\n",
    "            # post-stage (required as using pre-activation blocks)\n",
    "            net.add(nn.BatchNorm(epsilon=2e-5))\n",
    "            net.add(nn.Activation('relu'))\n",
    "            net.add(nn.GlobalAvgPool2D())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markdown_table(data):\n",
    "    content = \"\"\n",
    "    content += \"Key  | Value\" + \"\\n\"\n",
    "    content += \"-----|-----\" + \"\\n\"\n",
    "    for key, value in data.items():\n",
    "        content += \"{} | {}\".format(key, value) + \"\\n\"\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, label):\n",
    "    output_argmax = output.argmax(axis=1).astype('int32')\n",
    "    label_argmax = label.astype('int32')\n",
    "    equal = output_argmax==label_argmax\n",
    "    accuracy = mx.nd.mean(equal.astype('float32')).asscalar()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def evaluate_accuracy(valid_data, model, ctx):\n",
    "    acc = 0.\n",
    "    count = 0\n",
    "    for batch_idx, (data, label) in enumerate(valid_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = model(data)\n",
    "        acc = acc + accuracy(output, label)\n",
    "        count += 1\n",
    "    return acc / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(train_dataloader, test_dataloader, optimizer, description):\n",
    "    run_id = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\") + \"/\" + description\n",
    "    writer = SummaryWriter(logdir=os.path.join(\"./logs/cifar10\", run_id))\n",
    "\n",
    "    ctx = mx.gpu()\n",
    "    kvstore = \"device\"\n",
    "    \n",
    "    net = ResNet(num_classes=10)\n",
    "    # lazy initialize parameters\n",
    "    net.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "    trainer = mx.gluon.Trainer(params=net.collect_params(), optimizer=optimizer, kvstore=kvstore)\n",
    "\n",
    "    train_metric = mx.metric.Accuracy()\n",
    "    loss_fn = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    run_description = markdown_table({\n",
    "        \"batch_size\": train_dataloader._batch_sampler._batch_size,\n",
    "        \"optimizer\": type(optimizer),\n",
    "        \"optimizer_momentum\": optimizer.momentum,\n",
    "        \"optimizer_wd\": optimizer.wd\n",
    "    })\n",
    "    writer.add_text(tag='run_description', text=run_description, global_step=0)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for batch_idx, (data_batch, label_batch) in enumerate(train_dataloader, start=1):\n",
    "            # move to required context (e.g. gpu)\n",
    "            data_batch = data_batch.as_in_context(ctx)\n",
    "            label_batch = label_batch.as_in_context(ctx)\n",
    "            # take forward and backward pass\n",
    "            with mx.autograd.record():\n",
    "                pred_batch = net(data_batch)\n",
    "                loss = loss_fn(pred_batch, label_batch)\n",
    "            loss.backward()\n",
    "            bs = data_batch.shape[0]\n",
    "            trainer.step(bs)\n",
    "            train_metric.update(label_batch, pred_batch)\n",
    "\n",
    "        # mxboard logging at end of each epoch\n",
    "\n",
    "        ## sample of the images passed to network\n",
    "        adj_data_batch = (data_batch - data_batch.min())/(data_batch.max() - data_batch.min())\n",
    "        writer.add_image(tag=\"batch\", image=adj_data_batch, global_step=epoch)\n",
    "\n",
    "        ## histograms of input, output and loss\n",
    "        writer.add_histogram(tag='input', values=data_batch, global_step=epoch, bins=100)\n",
    "        writer.add_histogram(tag='output', values=pred_batch, global_step=epoch, bins=100)\n",
    "        writer.add_histogram(tag='loss', values=loss, global_step=epoch, bins=100)\n",
    "\n",
    "        ## learning rate\n",
    "        writer.add_scalar(tag=\"learning_rate\", value=trainer.learning_rate, global_step=epoch)\n",
    "        \n",
    "        ## training accuracy\n",
    "        _, trn_acc = train_metric.get()\n",
    "        writer.add_scalar(tag='accuracy/training', value=trn_acc * 100, global_step=epoch)\n",
    "        \n",
    "        ## test accuracy\n",
    "        test_acc = evaluate_accuracy(test_dataloader, net, ctx)\n",
    "        writer.add_scalar(tag='accuracy/testing', value=test_acc * 100, global_step=epoch)\n",
    "        \n",
    "        print(\"Completed epoch {}\".format(epoch))\n",
    "\n",
    "    writer.close()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 1\n",
      "Completed epoch 2\n",
      "Completed epoch 3\n",
      "Completed epoch 4\n",
      "Completed epoch 5\n",
      "Completed epoch 6\n",
      "Completed epoch 7\n",
      "Completed epoch 8\n",
      "Completed epoch 9\n",
      "Completed epoch 10\n"
     ]
    }
   ],
   "source": [
    "lr_schedule = lambda iteration: min(iteration ** -0.5, iteration * 782 ** -1.5)\n",
    "optimizer = mx.optimizer.SGD(lr_scheduler=lr_schedule)\n",
    "trained_net = train_resnet(train_dataloader, test_dataloader, optimizer, description=\"baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update 1: Shuffle training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 1\n",
      "Completed epoch 2\n",
      "Completed epoch 3\n",
      "Completed epoch 4\n",
      "Completed epoch 5\n",
      "Completed epoch 6\n",
      "Completed epoch 7\n",
      "Completed epoch 8\n",
      "Completed epoch 9\n",
      "Completed epoch 10\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = mx.gluon.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "optimizer = mx.optimizer.SGD(lr_scheduler=lr_schedule) # reset optimizer state (for momentum, lr schedule, etc)\n",
    "trained_net = train_resnet(train_dataloader, test_dataloader, optimizer, description=\"w_shuffle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update 2: Increase batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 1\n",
      "Completed epoch 2\n",
      "Completed epoch 3\n",
      "Completed epoch 4\n",
      "Completed epoch 5\n",
      "Completed epoch 6\n",
      "Completed epoch 7\n",
      "Completed epoch 8\n",
      "Completed epoch 9\n",
      "Completed epoch 10\n"
     ]
    }
   ],
   "source": [
    "batch_size = batch_size * 4\n",
    "train_dataloader = mx.gluon.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_dataloader = mx.gluon.data.DataLoader(test_dataset, batch_size)\n",
    "\n",
    "# lr_schedule = lambda iteration: min(iteration ** -0.5, iteration * 782 ** -1.5)\n",
    "new_lr_schedule = lambda iteration: lr_schedule(iteration*4) * 4\n",
    "optimizer = mx.optimizer.SGD(lr_scheduler=new_lr_schedule)\n",
    "\n",
    "trained_net = train_resnet(train_dataloader, test_dataloader, optimizer, description=\"inc_bs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update 3: Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 1\n",
      "Completed epoch 2\n",
      "Completed epoch 3\n",
      "Completed epoch 4\n",
      "Completed epoch 5\n",
      "Completed epoch 6\n",
      "Completed epoch 7\n",
      "Completed epoch 8\n",
      "Completed epoch 9\n",
      "Completed epoch 10\n"
     ]
    }
   ],
   "source": [
    "transform_fn = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465],[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "train_dataset = mx.gluon.data.vision.CIFAR10(train=True).transform_first(transform_fn)\n",
    "train_dataloader = mx.gluon.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_dataset = mx.gluon.data.vision.CIFAR10(train=False).transform_first(transform_fn)\n",
    "test_dataloader = mx.gluon.data.DataLoader(test_dataset, batch_size)\n",
    "\n",
    "optimizer = mx.optimizer.SGD(lr_scheduler=new_lr_schedule)\n",
    "\n",
    "trained_net = train_resnet(train_dataloader, test_dataloader, optimizer, description=\"normalized_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxb_1_tb1.6)",
   "language": "python",
   "name": "conda_mxb_1_tb1.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
